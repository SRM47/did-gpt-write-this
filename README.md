# did-gpt-write-this
Assess whether we can distinguish between large language model generated text from human written text.

## Hypothesis
Use an LM to calculate the perplexity of the text generated by itself. Text generated by the _LM itself_ will have a lower perplexity than other text not generated by the LM.

## Questions
- Can we use language model A to test the perplexity of text generated by language model B?
- What are all the cross relationships between "perplexity of LM-B text from LM-A"?
- How do we classify LM generated text from human written text?

## Statistical/Probabilistic Language Models
- Bigram LM
- Trigram LM

## Neural Language Models
- GPT-X
- BLOOM
